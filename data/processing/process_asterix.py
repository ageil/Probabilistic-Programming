import pickle
from glob import glob

from asterixdb.asterixdb import AsterixConnection

# connect to AsterixDB
con = AsterixConnection(server="http://localhost", port=19002)

# Reddit #

# Load all reddit submissions with direct load
# (note BigQuery gives `int` as `string` so have to cast):

rt_paths = sorted(glob("/Users/ageil/Github/FactMap/Data/reddit/new/*.json"))
combined = ""
for p in rt_paths:
    combined += "localhost://" + p + ","
combined = combined[:-1]  # remove last ,

q = """
    USE FactMap;

    DROP TYPE submissionTypeTemp IF EXISTS;
    CREATE TYPE submissionTypeTemp as {{
        uid: uuid
    }};

    DROP DATASET PostsTemp IF EXISTS;
    CREATE DATASET PostsTemp(submissionTypeTemp)
        PRIMARY KEY uid AUTOGENERATED;

    LOAD DATASET PostsTemp
    USING localfs (("path"="{0}"),("format"="json"));
    """.format(
    combined
)
response = con.query(q)

# Cast to intended data format and clean up temp table:
response = con.query(
    """
    USE FactMap;

    DROP DATASET posts IF EXISTS;

    CREATE DATASET posts(SubmissionType)
        PRIMARY KEY id;

    UPSERT INTO posts
    SELECT
        id,
        datetime_from_unix_time_in_secs(int(created_utc)) as created_utc,
        subreddit_id,
        subreddit,
        author,
        domain,
        int(score) as score,
        int(num_comments) as num_comments,
        title,
        url
    FROM PostsTemp p;

    DROP DATASET PostsTemp IF EXISTS;
    """
)
# note: use `UPSERT INTO` to protect against duplicate ids


# ClaimReview #

# Load all claims, generate uid:
response = con.query(
    """
    USE FactMap;

    DROP TYPE ReviewsType IF EXISTS;
    CREATE TYPE ReviewsType as {
        uid: uuid
    };

    DROP DATASET claims IF EXISTS;
    CREATE DATASET claims(ReviewsType)
        PRIMARY KEY uid AUTOGENERATED;

    LOAD DATASET claims
    USING localfs (("path"="localhost:///Users/ageil/Github/FactMap/Data/
    claimreviews/claims_2020_combined.json"),("format"="json"));
    """
)

# Clean up formatting:
response = con.query(
    """
    USE FactMap;

    CREATE DATASET reviews(ReviewsType)
        PRIMARY KEY uid;

    INSERT INTO reviews
    SELECT
        uid,
        reviewUrl,
        claimReviewed,
        countries,
        claimReviewed_en,
        datetime_from_unix_time_in_secs(claimDate) as claimDate,
        datetime_from_unix_time_in_secs(reviewDate) as reviewDate,
        reviewAuthor,
        reviewRating,
        claimAuthor,
        tagsRaw,
        tagsNamed,
        reviewTitle
    FROM claims c;

    DROP DATASET claims IF EXISTS;
    """
)

# Save subset with valid/invalid ratings:
rated = con.query(
    """
    USE FactMap;

    SELECT r.*
    FROM reviews r
    WHERE r.reviewRating.bestRating >= r.reviewRating.ratingValue
    AND r.reviewRating.ratingValue >= r.reviewRating.worstRating
    AND r.reviewRating.bestRating > r.reviewRating.worstRating;
"""
).results
unrated = con.query(
    """
    USE FactMap;

    SELECT r.*
    FROM reviews r
    WHERE NOT (r.reviewRating.bestRating >= r.reviewRating.ratingValue
    AND r.reviewRating.ratingValue >= r.reviewRating.worstRating
    AND r.reviewRating.bestRating > r.reviewRating.worstRating)
    OR is_null(r.reviewRating.ratingValue)
    OR is_null(r.reviewRating.worstRating)
    OR is_null(r.reviewRating.bestRating);
"""
).results

with open("/Users/ageil/Github/FactMap/RNN/data/rated_2020.pickle", "wb") as f:
    pickle.dump(rated, f, pickle.HIGHEST_PROTOCOL)
print("num rated:", len(rated))

with open("/Users/ageil/Github/FactMap/RNN/data/unrated.pickle", "wb") as f:
    pickle.dump(unrated, f, pickle.HIGHEST_PROTOCOL)
print("num unrated:", len(unrated))


# Joining #

# Hard join (claims = news articles)
# Join on full fake news source URL.
response = con.query(
    """
    USE FactMap;

    SET `compiler.joinmemory` "128MB";

    DROP DATASET urljoin IF EXISTS;
    DROP TYPE PostReviewType IF EXISTS;

    CREATE TYPE PostReviewType as {
        r: ReviewsType,
        p: SubmissionType
    };

    CREATE DATASET urljoin(PostReviewType)
        PRIMARY KEY r.uid, p.id;

    INSERT INTO urljoin
    SELECT *
    FROM posts p, reviews r
    WHERE r.claimAuthor.claimURL = p.url;
    """
)

response = con.query(
    """
    USE FactMap;

    SELECT u.*
    FROM urljoin u;
    """
)
print("Number of matches:", len(response.results))

# So how many unique claimreviews are represented?
response = con.query(
    """
    USE FactMap;

    SELECT count(distinct r.uid) as c
    FROM urljoin u
    LIMIT 1;
    """
)
print("Number of unique claims:", response.results[0]["c"])

# And how many of the ratings are numerically valid/invalid?
response = con.query(
    """
    USE FactMap;

    SELECT count(distinct r.uid) as c
    FROM urljoin u
    WHERE
    (r.reviewRating.worstRating < r.reviewRating.bestRating
    AND
    r.reviewRating.worstRating <= r.reviewRating.ratingValue
    AND
    r.reviewRating.ratingValue <= r.reviewRating.bestRating)
    LIMIT 1;
    """
)
print("Number of unique, valid numerical ratings:", response.results[0]["c"])

response = con.query(
    """
    USE FactMap;

    SELECT count(distinct r.uid) as c
    FROM urljoin u
    WHERE
    NOT
    (r.reviewRating.worstRating < r.reviewRating.bestRating
    AND
    r.reviewRating.worstRating <= r.reviewRating.ratingValue
    AND
    r.reviewRating.ratingValue <= r.reviewRating.bestRating)
    LIMIT 1;
    """
)
print("Number of unique, invalid numerical ratings:", response.results[0]["c"])


# Hard join (review articles) #
response = con.query(
    """
    USE FactMap;

    SET `compiler.joinmemory` "128MB";

    DROP DATASET facturljoin IF EXISTS;

    CREATE DATASET facturljoin(PostReviewType)
        PRIMARY KEY r.uid, p.id;

    INSERT INTO facturljoin
    SELECT *
    FROM posts p, reviews r
    WHERE r.reviewUrl = p.url;
    """
)

response = con.query(
    """
    USE FactMap;

    SELECT u.*
    FROM facturljoin u;
    """
)
print("Number of matches:", len(response.results))

response = con.query(
    """
    USE FactMap;

    SELECT COUNT(DISTINCT r.uid) unique_claims
    FROM facturljoin f;
    """
)
print("Number of unique claims", response.results[0]["unique_claims"])

# So there's actually more corrected news posted to reddit than fake news!

# Most cross-posts
response = con.query(
    """
    USE FactMap;

    SELECT COUNT(r.uid) as c, g
    FROM facturljoin f
    GROUP BY r.uid
    GROUP AS g
    ORDER BY c desc
    LIMIT 5;
    """
).results

for i in response:
    print("Number of cross-posts:", i["c"])

    subs = [p["f"]["p"]["subreddit"] for p in i["g"]]
    print("Subreddits:\n", subs)
    print()

# On further inspection, no need for further fuzzy matching on review matches.


# Hard join w/ fuzzy join on Twitter/Wikipedia (claims) #

# More refined join, using hard URL join only except for wikipedia and twitter
# posts, since these URLs to these sites are not claim specific and may refer
# to multiple tweets/sections.

# Additional requirements for Twitter/Wikipedia posts include:
# - The post domain must contain either `wikipedia` or `twitter`.
# - Minimum caption length is 15 characters (cannot meaningfully distinguish
# claims below this threshold).
# - Similar-length claim and post titles must have at least 20% word tokens
# in common.
#     - Similar-length is defined as the difference in length between the two
#     titles does not exceed 20% of the shortest title.
#     - Different-length claim and post titles must be contained within one or
#     the other, changing at most 50% of the characters to generate a perfect
#     match to the containing title.
#     - Different-length is defined as the difference in length between the two
#     titles must be at least 20% of the shortest title.

# Create table for fuzzy URL join:
response = con.query(
    """
    USE FactMap;

    SET `compiler.joinmemory` "128MB";

    DROP DATASET fuzzyurljoin IF EXISTS;

    CREATE DATASET fuzzyurljoin(PostReviewType)
        PRIMARY KEY r.uid, p.id;
    """
)

# Get URL matches with same-length review and post titles (Jaccard sim. > 20%):
response = con.query(
    """
        USE FactMap;

        INSERT INTO fuzzyurljoin
        SELECT u.*
        FROM urljoin u
        WHERE
            (similarity_jaccard(word_tokens(lower(p.title)),
            word_tokens(lower(r.claimReviewed))) > 0.20
            OR similarity_jaccard(word_tokens(lower(p.title)),
            word_tokens(lower(r.claimReviewed_en))) > 0.20)
            AND (abs(length(r.claimReviewed) - length(p.title)) <=
                (array_min([length(r.claimReviewed), length(p.title)])) * 0.2)
            AND (array_min([length(r.claimReviewed), length(p.title)]) > 15)
            AND (contains(p.domain, "wikipedia")
            OR contains(p.domain, "twitter"));
        """
)

# Get URL matches with different-length review and post titles
# (edit distance > 0.5 * min(review or post title length)):
response = con.query(
    """
    USE FactMap;

    INSERT INTO fuzzyurljoin
    SELECT u.*
    FROM urljoin u
    WHERE
            (
                (edit_distance_contains(lower(p.title), lower(r.claimReviewed),
                length(r.claimReviewed) * 0.5)[0]
                    OR edit_distance_contains(lower(r.claimReviewed),
                    lower(p.title), length(p.title) * 0.5)[0])
                OR
                (edit_distance_contains(lower(p.title),
                lower(r.claimReviewed_en), length(r.claimReviewed_en) * 0.5)[0]
                    OR edit_distance_contains(lower(r.claimReviewed_en),
                    lower(p.title), length(p.title) * 0.5)[0])
            )
            AND (abs(length(r.claimReviewed) - length(p.title)) >
                (array_min([length(r.claimReviewed), length(p.title)])) * 0.2)
            AND (array_min([length(r.claimReviewed), length(p.title)]) > 15)
            AND (contains(p.domain, "wikipedia")
            OR contains(p.domain, "twitter"));
    """
)

# Add back all other url-joined pairs that are not linked to wiki or twitter:
response = con.query(
    """
    USE FactMap;

    INSERT INTO fuzzyurljoin
    SELECT u.*
    FROM urljoin u
    WHERE NOT (contains(p.domain, "wikipedia")
    OR contains(p.domain, "twitter"));
    """
)

# Count total number of (fuzzy) url joined matches:
response = con.query(
    """
    USE FactMap;

    SELECT COUNT(*) as total
    FROM fuzzyurljoin u;
    """
)
print("Number of matches:", response.results[0]["total"])

response = con.query(
    """
    USE FactMap;

    SELECT COUNT(DISTINCT r.uid) unique_claims
    FROM fuzzyurljoin f;
    """
)
print("Number of unique claims", response.results[0]["unique_claims"])

response = con.query(
    """
    USE FactMap;

    SELECT count(distinct r.uid) as c
    FROM fuzzyurljoin u
    WHERE
    (r.reviewRating.worstRating < r.reviewRating.bestRating
    AND
    r.reviewRating.worstRating <= r.reviewRating.ratingValue
    AND
    r.reviewRating.ratingValue <= r.reviewRating.bestRating)
    LIMIT 1;
    """
)
print("Number of unique, valid numerical ratings:", response.results[0]["c"])

response = con.query(
    """
    USE FactMap;

    SELECT count(distinct r.uid) as c
    FROM fuzzyurljoin u
    WHERE
    NOT
    (r.reviewRating.worstRating < r.reviewRating.bestRating
    AND
    r.reviewRating.worstRating <= r.reviewRating.ratingValue
    AND
    r.reviewRating.ratingValue <= r.reviewRating.bestRating)
    LIMIT 1;
    """
)
print("Number of unique, invalid numerical ratings:", response.results[0]["c"])
